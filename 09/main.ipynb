{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. hét / Prológus\n",
    "\n",
    "A mai órán a következőkről lesz szó:\n",
    "- Elméleti bevezetés a Deep Learningbe (~ 20 perc)\n",
    "- Regresszió mélytanulás segítségével: California Housing Prices (~ 30 perc)\n",
    "- Klasszifikáció mélytanulás segítségével: MNIST (~ 30 perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. hét / I. Elméleti bevezetés a Deep Learningbe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ismétlés: Machine Learning feladatok megoldása"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jellemzően a gépi tanulás feladatai az alábbi lépésekből épülnek fel:\n",
    "1. **Adatgyűjtés**, mérés:<span style=\"color:red\"> Az adattudományok nem fognak működni adatok nélkül!</span> Fontos, hogy a mérések elvégzése előtt tegyünk becsléseket arra vonatkozóan, hogy mennyire sok adatra van szükségünk és mekkora paramétertérrel akarunk dolgozni! Minél több paramétert mérünk, annál több adatra van szükségünk! (A paramétertér dimenziójával exponenciálisan növekszik a szükséges adattér \"térfogata\".)\n",
    "\n",
    "2. **Adatfeltérképezés**: alapvető - statisztikai eszközökkel történő - megismerkedés az adatbázisunkkal. Például ábrázoljuk az egyes attribútumok eloszlását, szórását, vagy kirajzoljuk a térbeli és időbeli folyamatokat.\n",
    "\n",
    "3. ! **Adatok előkészítése**, data preprocessing: az adathalmaz előkészítése a modellek betanításának megfelelően. Ez maga a \"kreatív agya\" az adattudományoknak, amikor a nyers adatbázisból kiszűrjük a számunkra lényeges részeket. Az adat minőségi paramétereitől függően teljesen eltérő eszközöket igényel: például egy képet gyakran fekete-fehérre átállítunk, átméretezünk, megkeressük és kiemeljük rajta az éleket, hogy kizárólag a lényeges információtartalommal rendelkező adattal dolgozzon az algoritmusunk. Minél ügyesebben tudjuk elvégezni az adathalmaz előkészítését, annál több nehézségtől mentjük meg magunkat meg a tanítás során!\n",
    "\n",
    "4. **Modellválasztás**: a tanító modell meghatározása. (*Megjegyzés: valójában a modellt ismernünk kell már a preprocessing előtt, elvégre a válaszott modellre fogjuk optimalizálni a nyers adathalmazt!*) Ez mindig azzal kezdődik, hogy definiáljuk a problémát (például klasszifikációt, vagy regressziót akarunk-e csinálni) és ennek megfelelően keresünk a szakirodalomban megfelelő megoldásokat. Ezek alapján összeállítjuk a modellünket.\n",
    "\n",
    "5. **Modell illesztés**, tanítás: a választott modell betanítása.\n",
    "\n",
    "6. **Kiértékelés**: a tanító folyamat kiértékelése megadott teljesítménymetrikák (például *precision*, *recall*, vagy *accuracy*) alapján. Érdemes ezek változását is ábrázolni a tanítás során.\n",
    "\n",
    "7. **Modell paramétereinek finomhangolása**: az előfeldolgozás, vagy a választott modellünk módosítása a teljesítménymetrikák szerint; illetve a tanítás során fellépő rendellenességek és anomáliák feltérképezése. Gyakran előfordul, hogy ugyan működik is a modellünk, de esetleg nem elég pontos, nem elég megbízható, nem tud eléggé általánosítani, túltanult. Ilyenkor az első négy lépésen visszafelé érdemes elkezdeni végighaladni és onnan újrakezdve optimalizálni a modellt. \n",
    "\n",
    "8. **Alkalmazás**: az előre meghatározott célnak megfelelő modell telepítése."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A perceptron modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron](perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. hét / II. Regresszió: California Housing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az alábbiakban meg fogunk ismerkedni a neurális hálók egy klasszikus \"Hello World!\" feladatával.\n",
    "\n",
    "Adott egy adathalmazunk, amely a California állambeli házak árát tartalmazza 8 paraméter függvényében. Feladatunk az, hogy készítsünk egy olyan matematikai modellt, amely képes predikciókat tenni arra vonatkozóan, hogy az adatbázisba egy újonnan bekerülő ingatlant mennyi pénzbe kerülhet.\n",
    "\n",
    "Az adatbázis elérhető az [alábbi linken](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Megoldás**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szükséges framework telepítése:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szükséges importok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nyolc = 9\n",
    "np.random.seed(nyolc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adatgyűjtés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az adatgyűjtés problémáját a megfelelő szaktárgyak fedik le, mint például a Méréstechnika, Szenzor- és aktuátortechnika, Mikrovezérlők programozása, illetve egyes specializációkon ebben még részletesebben is elmélyülhetnek az érdeklődők.\n",
    "\n",
    "Jelen kurzus nem fektet különösebb hangsúlyt arra, hogy a résztvevők a mérésadatgyűjtés és jelfeldolgozás eszközeiben jobban elmélyedjenek. Ennek elsődleges oka, hogy ezekről már külön-külön is egy teljes féléves anyagot össze lehetne állítani. A másik nagyon fontos dolog, hogy mérni (különösen akkora méretekben, mint ahogy azt az adattudományok igénylik! Emlékeztetőül: ha a vizsgált attribútumok száma $n$, akkor a szükséges adathalmaz mérete $2^n$-nel arányos!) roppant **költséges** folyamat! Gondoljunk csak arra, hogy a pontos mérésekhez mennyire sok szenzorra és mindeneklőtt üzemórára van szükségünk.\n",
    "\n",
    "Ezen megfontolásokkal a tárgy keretein belül mindig feltételezzük, hogy **a megoldani kívánt feladat szempontjából alkalmas adatbázis áll rendelkezésre!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adatfeltérképezés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jellemzően az adatfeltérképezés során rengeteg ábrát és statisztikai eszközt érdemes ráereszteni az adatbázisunkra. Mivel jelen esetben egy nagyon jól ismert adathalmazról van szó, ettől most eltekinthetünk. Részletes leírást az [alábbi linken](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html) kaphatunk.\n",
    "\n",
    "Többek között azért fontos ez, hogy például ki tudjunk választani olyan attribútumokat, vagy változókat amelyek feltehetően teljesen irrelevánsak. Vagy éppen előre megjósolhassuk, hogy vajon mik lesznek a fontos paraméterek. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Az adathalmaz felbontása**\n",
    "\n",
    "Nagyon fontos lépés, hogy miután preprocesszáltuk a nyers adatot, utána osszuk fel három különböző részre. Ezek mind más funkciót fognak betölteni a *tanítás* során!\n",
    "- **Training set**, vagy *tanító adathalmaz*: az adatnak az a része, amelyet ténylegesen fel fog dolgozni a neurális háló. Elsősorban ez az adathalmaz fogja definiálni a neurális háló főbb paramétereit. \n",
    "\n",
    "- **Validation set**, vagy *validációs adathalmaz*: a kiindulási adathalmaz azon része, amelyet ugyan tanítás közben folyamatosan lát a neurális háló, de az ebből szerzett információt csak a modell finomhangolására és hiperparaméteroptimalizálására használja. Például a validációs adathalmaz segítségével könnyen ellenőrizhetővé válik, hogy hány *epoch* után érdemes leállítani a tanítást, hogy elkerüljük az overfitting-et! (A validációs adathalmazt definiálni nem kötelező, de az esetek túlnyomó részében több, mint érdemes!)\n",
    "\n",
    "- **Test set**, vagy *tesztelési adathalmaz*: azon adathalmaz, amelyet a tanítás végeztével arra használunk, hogy a neurális háló metrikáit kiértékeljük. Ezt az adathalmazt tanítás során semmiképp sem láthatja a neurális háló, ugyanis az meghamisítaná a modell teljesítményét!\n",
    "\n",
    "Ökölszabályként érdemes megjegyezni, hogy jellemzően 60%-20%-20%, 70%-15%-15% arányokat szoktunk választani! Viszont fontos emellett, hogy ezen halmazoknak nem is az aránya, sokkal inkább a homogenitása ami mérvadó! Nagyon fontos, hogy mindhárom halmazban minél ideálisabban legyen reprezentálva a teljes adathalmaz, különben sok anomáliával szembesülhetünk! *(Például fontos, hogy egy rendezett adathalmazt ne egymás utáni részekre osszuk fel, hanem véletlenszerűen válasszuk ki az egyes elemeket! Ha nem így teszünk, akkor könnyen előfordulhat, hogy tanítás során például közel 100%-os accuracy értéket kapunk, a tesztelés során pedig ettől jelentősen kevesebb érték jelenik meg.)*\n",
    "\n",
    "**3.2 Az adathalmaz standardizálása**\n",
    "\n",
    "Szigorúan az adathalmaz felosztása után érdemes egyenként a tanító, validációs és tesztelési adathalmazainkat standardizálni. Ez azt jelenti, hogy a $\\mathbf{X}$ adathalmazunk elemeit **normalizál**juk, azaz úgy transzformáljuk, hogy az $E$ **várható érték**e (jelen esetben: számtani közép, amit szokás még $\\mu$-vel is jelölni) **zérus**, illetve a $\\sigma$ **szórás**a **egységnyi** legyen! \n",
    "$\\begin{equation} \\hat{\\mathbf{X}} = \\frac{\\mathbf{X}-E}{\\sigma} \\end{equation}$\n",
    "Erre azért van szükség, hogy egyrészt ne kelljen feleslegesen nagy számokkal dolgoznia a neurális hálónak, másrészt pedig azért, hogy könnyebben tudjon általánosítani a modell.\n",
    "\n",
    "*(Megjegyzés: miért nem mindegy, hogy az adathalmaz felbontása előtt, vagy után standardizáljuk az adatot? Melyik adathalmaznak és melyik aspektusa sérülne, ha fordítva járnánk el?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = california_housing.frame.values\n",
    "\n",
    "# Tanító-validációs-tesztelő adathalmazok méretének definiálása\n",
    "test_split = \n",
    "valid_split =\n",
    "\n",
    "X = \n",
    "Y = \n",
    "\n",
    "# A megfelelő arányokban felosztjuk az egyes adathalmazokat\n",
    "v_index = \n",
    "t_index = \n",
    "\n",
    "# Tesztelő adat: 90%-tól végig\n",
    "X_test =\n",
    "Y_test = \n",
    "\n",
    "# Validációs adat: 80%-90%-ig\n",
    "X_valid =\n",
    "Y_valid =\n",
    "\n",
    "# Tanító adat: elejétől a 80%-ig\n",
    "X =\n",
    "Y = \n",
    "# Normalizáljuk az adatot (σ = 1, E = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modellválasztás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sgd.jpg](sgd.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback függvények definiálása\n",
    "\n",
    "# Modell felépítése\n",
    "\n",
    "# Model tanításának hiperparaméterei\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modell illesztés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miután definiáltuk az összes szükséges paramétert a modellünkben, kezdődhet maga a tanítási folyamat. Fontos megjegyezni, hogy ez egy masszívan számításigényes folyamat, ezért egy-egy tanítás könnyűszerrel igénybevehet több órát, de akár napokat is! Emiatt mindig érdemes egyszerűbb modelleket felépíteni, kevesebb réteggel és kevesebb neuronnal, a gyors betanulás érdekében. Ezen csak akkor érdemes javítani, ha a cél szempontjából jelentősen alulteljesít a modellünk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Kiértékelés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Betöltjük az elmentett modellt\n",
    "\n",
    "# Predikálunk a teszt adatbázison\n",
    "\n",
    "# Kiértékeljük a tesztelési adatbázison a modell hibáját\n",
    "test_err = \n",
    "\n",
    "print(\"\\nTeszt hiba: %f\" % (test_err))\n",
    "print(f\"Ez az jelenti, hogy sqrt({test_err:0.3f}))={math.sqrt(test_err):0.3f}-et hibáz átlagosan a modell a teszt adatokon.\")\n",
    "print(f\"Ez annyit jelent, hogy átlagosan {math.sqrt(test_err)*100000:0.0f}$-t téved a modellünk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "fig = sns.regplot(x=Y_test, y=preds.reshape(-1));\n",
    "# fig.set(xlim=(10,30),ylim=(10,30))\n",
    "plt.xlabel(\"Ground truth [$ 100 000]\")\n",
    "plt.ylabel(\"Predictions [$ 100 000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. A modell finomhangolása"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jelen esetben megállapíthatjuk, hogy a modell tanítása során nem jelentkezett semmilyen anomálisa, vagy rendellenesség. Nem állapítható meg túltanulás sem, ezért a kapott modellt **elfogadom**.\n",
    "\n",
    "Ezen felül még érdemes figyelni, hogy mennyire gyorsan tud predikálni a modell és figyelembe venni, hogy vajon emellett képesek vagyunk-e valós idejű alkalmazást létrehozni. Ha nem, akkor célszerű egy egyszerűbb modellt választani."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Alkalmazás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha már nem tartunk igényt a modell további finomhangolására, akkor a `weights.keras` file-t elmentve készen állunk arra, hogy alkalmazásba ültessük a neurális hálónkat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. hét / III. Klasszifikáció: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jelen példában ismételten egy \"Hello World\" típusú feladatot fogunk megvizsgálni, viszont jelen esetben már nem egy MLP (Multi Layer Perceptron) modellt fogunk segítségül hívni, hanem egy úgynevezett Konvolúciós Neurális Hálót, röviden CNN-t!\n",
    "\n",
    "A feladat a következő: adott az MNIST adatbázis, amely rengeteg kézzel írott számjegyről tartalmaz 256x256-os felbontásban képeket. A feladatunk az, hogy készítsünk egy olyan modellt, amely képes egy újonnan beadott képről eldönteni, hogy azon milyen számjegy található!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mielőtt mélyebbre áskálódnánk, fontos tisztázni, hogy miből is épül fel egy CNN! Tekintsük az alábbi példát: \n",
    "![cnn](2DCNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Betöltjük az adatbázist - itt eleve szét van szedve tanító és teszt adathalamzra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =\n",
    "x_test =\n",
    "\n",
    "x_train = \n",
    "x_test = \n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Embedding\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "patience=10\n",
    "early_stopping=EarlyStopping(patience=patience, verbose=1)\n",
    "checkpointer=ModelCheckpoint(filepath='model.keras', save_best_only=True, verbose=1)\n",
    "\n",
    "# tb = TensorBoard(log_dir='logs', histogram_freq=1, write_graph=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "network_history = model.fit(x_train, y_train, batch_size=128, epochs=30, verbose=1, validation_split=0.2, callbacks=[early_stopping, checkpointer, tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"mopdel.keras\")\n",
    "test_err = model.evaluate(x_test,y_test)\n",
    "print(\"Teszt hiba:\", test_err[0], \"Teszt pontosság:\", test_err[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix,classification_report\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred,1)\n",
    "y_true = np.argmax(y_test,1)\n",
    "\n",
    "print(\"test accuracy: %g\" %(accuracy_score(y_true, y_pred)))\n",
    "print(\"Precision\", precision_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"Recall\", recall_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"f1_score\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"\\nKonfúziós mátrix: \")\n",
    "conf=confusion_matrix(y_true, y_pred)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(conf, annot=True, fmt='d', vmax=20, cmap='Blues') # a vmax paraméterrel állítjuk be, hogy milyen értéktartományban jelenítse meg az adatokat\n",
    "ax.set(xlabel='Predicted Label',\n",
    "       ylabel='True label');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
